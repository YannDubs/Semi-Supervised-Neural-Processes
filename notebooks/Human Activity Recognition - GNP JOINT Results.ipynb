{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition - SSL JOINT Results\n",
    "\n",
    "Last Update : 31 July 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 8\n",
    "# Nota Bene : notebooks don't deallocate GPU memory\n",
    "IS_FORCE_CPU = True # can also be set in the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/conv\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
       ".prompt display:none;}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# CENTER PLOTS\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\" <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
    ".prompt display:none;}  </style>\"\"\"))\n",
    "\n",
    "import os\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skssl.transformers.neuralproc.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "from utils.data.tsdata import get_timeseries_dataset, SparseMultiTimeSeriesDataset\n",
    "\n",
    "get_cntxt_trgt_test = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.1, max_n_indcs=0.5),\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt_feat = CntxtTrgtGetter(contexts_getter=get_all_indcs,\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.99),\n",
    "                                 targets_getter=GetRandomIndcs(min_n_indcs=0.5, max_n_indcs=0.99),\n",
    "                                 is_add_cntxts_to_trgts=False)  # don't context points to tagrtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_both = get_timeseries_dataset(\"har\")(split=\"both\")\n",
    "\n",
    "def cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=False):\n",
    "    def mycollate(batch):\n",
    "        min_length = min([v.size(0) for b in batch for k,v in b[0].items() if \"X\" in k])\n",
    "        # chose first min_legth of each (assumes that randomized)\n",
    "        \n",
    "        batch = [({k:v[:min_length, ...] for k,v in b[0].items()}, b[1]) for b in batch]        \n",
    "        collated = torch.utils.data.dataloader.default_collate(batch)\n",
    "        \n",
    "        X = collated[0][\"X\"]\n",
    "        y = collated[0][\"y\"]\n",
    "        \n",
    "        if is_repeat_batch:\n",
    "            \n",
    "            X = torch.cat([X,X], dim=0)\n",
    "            y = torch.cat([y,y], dim=0)\n",
    "            collated[1] = torch.cat([collated[1], collated[1]], dim=0) # targets\n",
    "        \n",
    "        collated[0][\"X\"], collated[0][\"y\"], collated[0][\"X_trgt\"], collated[0][\"y_trgt\"] = get_cntxt_trgt(X, y)\n",
    "        \n",
    "        return collated\n",
    "    return mycollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DIM = 1  # 1D spatial input (although actually 2 but the first is for sparse channels)\n",
    "Y_DIM = data_both.data.shape[-1] # multiple channels\n",
    "N_TARGETS = len(np.unique(data_both.targets))\n",
    "\n",
    "sampling_percentages = [1]\n",
    "label_percentages = [0.01, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from skssl.transformers import GlobalNeuralProcess, NeuralProcessLoss, AttentiveNeuralProcess, NeuralProcessSSLLoss\n",
    "from skssl.utils.helpers import rescale_range\n",
    "from skssl.predefined import UnetCNN, CNN, MLP, SparseSetConv, SetConv, MlpRBF, GaussianRBF, BatchSparseSetConv\n",
    "from skssl.transformers.neuralproc.datasplit import precomputed_cntxt_trgt_split\n",
    "from utils.helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "models = {}\n",
    "\n",
    "min_std=5e-3\n",
    "\n",
    "unet = partial(UnetCNN,\n",
    "               Conv=torch.nn.Conv1d,\n",
    "               Pool=torch.nn.MaxPool1d,\n",
    "               upsample_mode=\"linear\",\n",
    "               n_layers=18,\n",
    "               is_double_conv=True,\n",
    "               is_depth_separable=True,\n",
    "               Normalization=torch.nn.BatchNorm1d,\n",
    "               is_chan_last=True,\n",
    "               bottleneck=None,\n",
    "               kernel_size=7,\n",
    "               max_nchannels=256,\n",
    "              is_force_same_bottleneck=True,\n",
    "               _is_summary=True,\n",
    "              )\n",
    "\n",
    "kwargs = dict(x_dim=X_DIM, \n",
    "              y_dim=Y_DIM,\n",
    "              min_std=min_std,\n",
    "                n_tmp_queries=128,\n",
    "                r_dim=64,\n",
    "              keys_to_tmp_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              TmpSelfAttn=unet,\n",
    "              tmp_to_queries_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              is_skip_tmp=False,\n",
    "              is_use_x=False,\n",
    "              get_cntxt_trgt=precomputed_cntxt_trgt_split,\n",
    "              is_encode_xy=False,\n",
    "             Classifier=partial(MLP, input_size=256+Y_DIM*4, output_size=N_TARGETS, \n",
    "                                dropout=0., hidden_size=128, n_hidden_layers=3, is_res=True))\n",
    "\n",
    "models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)\n",
    "\n",
    "kwargs_bis = deepcopy(kwargs)\n",
    "kwargs_bis[\"Classifier\"] = None\n",
    "\n",
    "models[\"transformer_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssl_classifier_gnp_large_shared_bottleneck - N Param: 1078238\n",
      "transformer_gnp_large_shared_bottleneck - N Param: 1006936\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "for k,v in models.items():\n",
    "    print(k, \"- N Param:\", count_parameters(v()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_(models, sampling_percentages):\n",
    "    # ALREADY INITALIZE TO BE ABLE TO LOAD\n",
    "    models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)()\n",
    "\n",
    "    kwargs_bis = deepcopy(kwargs)\n",
    "    kwargs_bis[\"Classifier\"] = None\n",
    "\n",
    "    models[\"transformer_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)()\n",
    "\n",
    "    # load all transformers\n",
    "    loaded_models = {}\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for k, m in models.items():\n",
    "            if \"transformer\" not in k:\n",
    "                continue\n",
    "\n",
    "            out = train_models_({\"{}%har\".format(int(sampling_perc*100)): \n",
    "                                                (None, None)}, \n",
    "                                  {k :m },\n",
    "                                   chckpnt_dirname=chckpnt_dirname_old,\n",
    "                                seed=None,\n",
    "                                   is_retrain=False)\n",
    "\n",
    "            pretrained_model = out[list(out.keys())[0]].module_\n",
    "            model_dict = models[k.replace(\"transformer\", \"ssl_classifier\")].state_dict()\n",
    "            model_dict.update(pretrained_model.state_dict())\n",
    "            models[k.replace(\"transformer\", \"ssl_classifier\")].load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntbks_helpers import train_models_\n",
    "from skorch.dataset import CVSplit\n",
    "from utils.data.ssldata import get_train_dev_test_ssl\n",
    "import random\n",
    "\n",
    "N_EPOCHS = 100 \n",
    "BATCH_SIZE = 32\n",
    "IS_RETRAIN = False # if false load precomputed\n",
    "chckpnt_dirname_old=\"results/challenge/har/\"\n",
    "chckpnt_dirname=\"results/challenge/har_new/\"\n",
    "\n",
    "from skssl.utils.helpers import HyperparameterInterpolator\n",
    "\n",
    "n_steps_per_epoch = len(data_both)//BATCH_SIZE\n",
    "get_lambda_clf=HyperparameterInterpolator(1, 10, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "7352\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug best epoch: 1 val_loss: 0.5792485174988745\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "7352\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug best epoch: 20 val_loss: 0.27427897649091754\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "7352\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug best epoch: 4 val_loss: 0.534082076773629\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "7352\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug best epoch: 8 val_loss: 0.18001823478688497\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "7352\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug best epoch: 7 val_loss: 0.5398399174557243\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "7352\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug best epoch: 4 val_loss: 0.20004334868841103\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=False)\n",
    "            print(len(data_train))\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run): \n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_noaug\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: 0.5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "14530\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune best epoch: 4 val_loss: 0.7066817718074813\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "12504\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune best epoch: 1 val_loss: 0.1426355674544359\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "14530\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune best epoch: 1 val_loss: 0.7923410251903502\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "12504\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune best epoch: 1 val_loss: 0.14788568104815167\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "14530\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune best epoch: 3 val_loss: 0.7332448063763999\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "12504\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune best epoch: 1 val_loss: 0.16581127908135174\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=True)\n",
    "            print(len(data_train))\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run): \n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: 0.5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Neg Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons best epoch: 1 val_loss: 0.5842806758175149\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons best epoch: 1 val_loss: 0.16748589485590443\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons best epoch: 1 val_loss: 0.6183924950055038\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons best epoch: 2 val_loss: 0.1616656260042232\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons best epoch: 1 val_loss: 0.6301195088345438\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons best epoch: 1 val_loss: 0.17687109902547107\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run):  \n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_nonegcons\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 1,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: 0.,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show number of steps for convergence becauseprobably no unsup is fine but not improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent best epoch: 1 val_loss: 0.6706024456477223\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent best epoch: 1 val_loss: 0.15668519885409757\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent best epoch: 1 val_loss: 0.6930868627258914\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent best epoch: 1 val_loss: 0.1925992108135756\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent best epoch: 1 val_loss: 0.667854422668623\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noent best epoch: 1 val_loss: 0.20008442545083252\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run):\n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_noent\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: 1.,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup best epoch: 1 val_loss: 0.6362027577476903\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup best epoch: 1 val_loss: 0.15670357431684714\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup best epoch: 1 val_loss: 0.6330289104488853\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup best epoch: 1 val_loss: 0.16049837192358557\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup best epoch: 1 val_loss: 0.7715492812423007\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup best epoch: 1 val_loss: 0.14915962489451892\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              dev_size=0,\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run):\n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_nounsup\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 0,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: .5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SSL Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly best epoch: 1 val_loss: 0.6721114105882182\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly best epoch: 16 val_loss: 0.1699327095346933\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly best epoch: 1 val_loss: 0.6721089704181528\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly best epoch: 7 val_loss: 0.15977995857611565\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly best epoch: 1 val_loss: 0.6721099129279752\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly best epoch: 7 val_loss: 0.18694964756106455\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              dev_size=0,\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run):\n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_sslonly\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=True,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: .5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sup Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly best epoch: 2 val_loss: 0.37116682323149275\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly best epoch: 2 val_loss: 0.11194615579274406\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly best epoch: 2 val_loss: 0.6973583848750674\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly best epoch: 4 val_loss: 0.11700662759186091\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly best epoch: 3 val_loss: 0.5974679222178128\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly best epoch: 6 val_loss: 0.1170268439825891\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              dev_size=0,\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = data_train.data[(data_train.targets!=-1).squeeze()]\n",
    "            data_train.targets = data_train.targets[(data_train.targets!=-1).squeeze()]\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run):\n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_suponly\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=True,\n",
    "                                                    get_lambda_unsup=lambda: 0,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: .5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sup Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom skorch.callbacks import Freezer, LRScheduler\\n\\nfor run in range(3):\\n    for sampling_perc in sampling_percentages:\\n        for label_perc in label_percentages:\\n            load_pretrained_(models, [sampling_perc])\\n\\n            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\\n\\n            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \\n                                                              n_labels=label_perc, \\n                                                              data_perc=sampling_perc,\\n                                                              seed=random.randint(0,10000),\\n                                                              dev_size=0,\\n                                                              is_augment=True)\\n\\n            # add test as unlabeled data\\n            data_train.data = data_train.data[(data_train.targets!=-1).squeeze()]\\n            data_train.targets = data_train.targets[(data_train.targets!=-1).squeeze()]\\n\\n            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run):\\n                                                (data_train, data_test)}, \\n                                  {k + \"_finetune_sup_vanilla\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \\n                                  criterion=partial(NeuralProcessSSLLoss, \\n                                                    get_lambda_sup=lambda : 1,\\n                                                    n_max_elements=None,\\n                                                    label_perc=None, # label perc is lower ebcause cocnat to test\\n                                                    min_sigma=min_std,\\n                                                    is_unsup_forall=False,\\n                                                    is_ssl_only=True,\\n                                                    get_lambda_unsup=lambda: 0,\\n                                                    ),\\n                                    patience=15,\\n                                  chckpnt_dirname=chckpnt_dirname,\\n                                  max_epochs=N_EPOCHS,\\n                                  batch_size=BATCH_SIZE,\\n                                  is_retrain=IS_RETRAIN,\\n                                               seed=None,\\n                                    is_monitor_acc=True,\\n                                  callbacks=[],\\n                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \\n                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\\n                                              ))\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              dev_size=0,\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = data_train.data[(data_train.targets!=-1).squeeze()]\n",
    "            data_train.targets = data_train.targets[(data_train.targets!=-1).squeeze()]\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run):\n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_sup_vanilla\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda : 1,\n",
    "                                                    n_max_elements=None,\n",
    "                                                    label_perc=None, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=True,\n",
    "                                                    get_lambda_unsup=lambda: 0,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                               seed=None,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Lambda CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda best epoch: 4 val_loss: 0.7258332047496442\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda best epoch: 1 val_loss: 0.16846257951812668\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda best epoch: 2 val_loss: 0.8610166228339354\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda best epoch: 1 val_loss: 0.14787215202754517\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda best epoch: 4 val_loss: 0.6764867800876655\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda best epoch: 2 val_loss: 0.13356725409915235\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run): \n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_nolambda\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: 1,\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: .5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Label Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale best epoch: 1 val_loss: 1.3277767340450073\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale best epoch: 18 val_loss: 0.2315054115376984\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale best epoch: 2 val_loss: 1.1983000453059698\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale best epoch: 2 val_loss: 0.18339065070230307\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale best epoch: 3 val_loss: 1.0681789091250675\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale best epoch: 1 val_loss: 0.26924419039016584\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run): \n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_nolabscale\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=None, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: 0.5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Element Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale best epoch: 1 val_loss: 0.5697207075480812\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale best epoch: 7 val_loss: 0.1779502336963541\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale best epoch: 2 val_loss: 0.7624972491981142\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale best epoch: 7 val_loss: 0.20060293440661897\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale best epoch: 1 val_loss: 0.649505396394114\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale best epoch: 7 val_loss: 0.15404326194903015\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "\n",
    "            load_pretrained_(models, [sampling_perc])\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run): \n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"_finetune_noelemscale\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=None,\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: 0.5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n",
      "har100%_lab1%_run0/ssl_classifier_gnp_large_shared_bottleneck best epoch: 1 val_loss: 0.8911615639651959\n",
      "\n",
      "--- Loading har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n",
      "har100%_lab10%_run0/ssl_classifier_gnp_large_shared_bottleneck best epoch: 2 val_loss: 0.2708716995238013\n",
      "\n",
      "--- Loading har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n",
      "har100%_lab1%_run1/ssl_classifier_gnp_large_shared_bottleneck best epoch: 2 val_loss: 0.5388232102667948\n",
      "\n",
      "--- Loading har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n",
      "har100%_lab10%_run1/ssl_classifier_gnp_large_shared_bottleneck best epoch: 5 val_loss: 0.1961393239824964\n",
      "\n",
      "--- Loading har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n",
      "har100%_lab1%_run2/ssl_classifier_gnp_large_shared_bottleneck best epoch: 1 val_loss: 0.8988694104252315\n",
      "\n",
      "--- Loading har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n",
      "har100%_lab10%_run2/ssl_classifier_gnp_large_shared_bottleneck best epoch: 2 val_loss: 0.19988441305480895\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)\n",
    "\n",
    "for run in range(3):\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for label_perc in label_percentages:\n",
    "\n",
    "            get_lambda_clf=HyperparameterInterpolator(1, 50, N_EPOCHS*n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "            data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                              n_labels=label_perc, \n",
    "                                                              data_perc=sampling_perc, \n",
    "                                                              dev_size=0,\n",
    "                                                              seed=random.randint(0,10000),\n",
    "                                                              is_augment=True)\n",
    "\n",
    "            # add test as unlabeled data\n",
    "            data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "            data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "            data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "\n",
    "            data_trainers.update(train_models_({\"har{}%_lab{}%_run{}\".format(int(sampling_perc*100), int(label_perc*100), run): \n",
    "                                                (data_train, data_test)}, \n",
    "                                  {k + \"\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                                  criterion=partial(NeuralProcessSSLLoss, \n",
    "                                                    get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                    n_max_elements=int(128*sampling_perc),\n",
    "                                                    label_perc=(label_perc * data_train.n_train)/data_train.n_total, # label perc is lower ebcause cocnat to test\n",
    "                                                    min_sigma=min_std,\n",
    "                                                    is_unsup_forall=False,\n",
    "                                                    is_ssl_only=False,\n",
    "                                                    get_lambda_unsup=lambda: 1,\n",
    "                                                     get_lambda_ent=lambda: 0.5,  # both do something similar\n",
    "                                                     get_lambda_neg_cons=lambda: 0.5,\n",
    "                                                    ),\n",
    "                                    patience=15,\n",
    "                                  chckpnt_dirname=chckpnt_dirname,\n",
    "                                  max_epochs=N_EPOCHS,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  is_retrain=IS_RETRAIN,\n",
    "                                    is_monitor_acc=True,\n",
    "                                  callbacks=[],\n",
    "                                  iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=True),  \n",
    "                                  iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                              ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile_converge_epoch(history, percentile=0.01):\n",
    "    best_loss = history[-1]['train_loss']\n",
    "    init_loss = history[0]['train_loss']\n",
    "    threshold = init_loss + (best_loss - init_loss) * (1 - percentile)\n",
    "    for h in history:\n",
    "        if h['train_loss'] <= threshold:\n",
    "            return h[\"epoch\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Models</th>\n",
       "      <th>... No Data Scaling</th>\n",
       "      <th>... No Entropy Minimization</th>\n",
       "      <th>... No Label Scaling</th>\n",
       "      <th>... No NP Loss</th>\n",
       "      <th>... No Negative Consistency</th>\n",
       "      <th>... No Oversampling</th>\n",
       "      <th>... No Supervised Annealing</th>\n",
       "      <th>... No Unsupervised Loss</th>\n",
       "      <th>... Pretrained</th>\n",
       "      <th>... Supervised Loss, No Rescaling</th>\n",
       "      <th>Joint UnetNP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label Percentage (%)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8823662481619726 +/-0.003628325965919103</td>\n",
       "      <td>0.8812351543942992 +/-0.0025618712030100964</td>\n",
       "      <td>0.893677185838706 +/-0.012967189672945594</td>\n",
       "      <td>0.8765976699468386 +/-0.004705948210092729</td>\n",
       "      <td>0.8849677638276213 +/-0.004450246708043437</td>\n",
       "      <td>0.8702635448478678 +/-0.0031345789979755935</td>\n",
       "      <td>0.8854202013346907 +/-0.011800295311159352</td>\n",
       "      <td>0.8716208573690759 +/-0.00463196363068507</td>\n",
       "      <td>0.8620065603438526 +/-0.01708925101078104</td>\n",
       "      <td>0.9040832485013008 +/-0.013515032337954007</td>\n",
       "      <td>0.883497341929646 +/-0.008286371635512803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9518154054971157 +/-0.0018892990712012139</td>\n",
       "      <td>0.9453681710213777 +/-0.001795555691255216</td>\n",
       "      <td>0.9533989367718583 +/-0.007567346989561189</td>\n",
       "      <td>0.9572446555819477 +/-0.0053329601783175095</td>\n",
       "      <td>0.9493269992082345 +/-0.001929501426222352</td>\n",
       "      <td>0.9347358896052483 +/-0.0016738659186686254</td>\n",
       "      <td>0.9565659993213438 +/-0.008332561355825331</td>\n",
       "      <td>0.952607171134487 +/-0.003737747232633912</td>\n",
       "      <td>0.9317950458092975 +/-0.006219987370147012</td>\n",
       "      <td>0.9639181088112205 +/-0.0049445748205845805</td>\n",
       "      <td>0.9519285148738831 +/-0.004754631728760044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Models                                        ... No Data Scaling  \\\n",
       "Label Percentage (%)                                                \n",
       "1                      0.8823662481619726 +/-0.003628325965919103   \n",
       "10                    0.9518154054971157 +/-0.0018892990712012139   \n",
       "\n",
       "Models                                ... No Entropy Minimization  \\\n",
       "Label Percentage (%)                                                \n",
       "1                     0.8812351543942992 +/-0.0025618712030100964   \n",
       "10                     0.9453681710213777 +/-0.001795555691255216   \n",
       "\n",
       "Models                                      ... No Label Scaling  \\\n",
       "Label Percentage (%)                                               \n",
       "1                      0.893677185838706 +/-0.012967189672945594   \n",
       "10                    0.9533989367718583 +/-0.007567346989561189   \n",
       "\n",
       "Models                                             ... No NP Loss  \\\n",
       "Label Percentage (%)                                                \n",
       "1                      0.8765976699468386 +/-0.004705948210092729   \n",
       "10                    0.9572446555819477 +/-0.0053329601783175095   \n",
       "\n",
       "Models                               ... No Negative Consistency  \\\n",
       "Label Percentage (%)                                               \n",
       "1                     0.8849677638276213 +/-0.004450246708043437   \n",
       "10                    0.9493269992082345 +/-0.001929501426222352   \n",
       "\n",
       "Models                                        ... No Oversampling  \\\n",
       "Label Percentage (%)                                                \n",
       "1                     0.8702635448478678 +/-0.0031345789979755935   \n",
       "10                    0.9347358896052483 +/-0.0016738659186686254   \n",
       "\n",
       "Models                               ... No Supervised Annealing  \\\n",
       "Label Percentage (%)                                               \n",
       "1                     0.8854202013346907 +/-0.011800295311159352   \n",
       "10                    0.9565659993213438 +/-0.008332561355825331   \n",
       "\n",
       "Models                                 ... No Unsupervised Loss  \\\n",
       "Label Percentage (%)                                              \n",
       "1                     0.8716208573690759 +/-0.00463196363068507   \n",
       "10                    0.952607171134487 +/-0.003737747232633912   \n",
       "\n",
       "Models                                            ... Pretrained  \\\n",
       "Label Percentage (%)                                               \n",
       "1                      0.8620065603438526 +/-0.01708925101078104   \n",
       "10                    0.9317950458092975 +/-0.006219987370147012   \n",
       "\n",
       "Models                          ... Supervised Loss, No Rescaling  \\\n",
       "Label Percentage (%)                                                \n",
       "1                      0.9040832485013008 +/-0.013515032337954007   \n",
       "10                    0.9639181088112205 +/-0.0049445748205845805   \n",
       "\n",
       "Models                                              Joint UnetNP  \n",
       "Label Percentage (%)                                              \n",
       "1                      0.883497341929646 +/-0.008286371635512803  \n",
       "10                    0.9519285148738831 +/-0.004754631728760044  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "out = pd.DataFrame({k:v.history[-1][\"valid_acc\"] for k,v in data_trainers.items()}, index=[\"Accuracy\"]\n",
    "                  ).T.reset_index()#name=\"accuracy\")\n",
    "splitted = out[\"index\"].str.split(\"/\", expand = True)\n",
    "out[\"meta\"] = splitted[0]\n",
    "out[\"models\"] = splitted[1]\n",
    "\n",
    "splitted2 = out[\"meta\"].str.split(\"_run\", expand = True)\n",
    "out[\"meta\"] = splitted2[0]\n",
    "out[\"run\"] = splitted2[1]\n",
    "\n",
    "splitted3 = out[\"meta\"].str.split(\"_lab\", expand = True)\n",
    "out[\"data sample\"] = splitted3[0].str.split(\"har\", expand = True)[1]\n",
    "out[\"lab\"] = splitted3[1]\n",
    "\n",
    "\n",
    "out.drop(columns =[\"index\"], inplace = True) \n",
    "\n",
    "out = out.groupby([\"models\", \"lab\"]).agg([\"mean\", \"std\"])\n",
    "\n",
    "out.reset_index(drop=False, inplace=True)\n",
    "\n",
    "out=out.replace({\"ssl_classifier_gnp_large_shared_bottleneck_finetune\":\"Joint UnetNP\", \n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck\":\"... Pretrained\", \n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug\":\"... No Oversampling\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale\":\"... No Data Scaling\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_noent\":\"... No Entropy Minimization\",\n",
    "                     \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale\": \"... No Label Scaling\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda\":\"... No Supervised Annealing\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons\":\"... No Negative Consistency\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup\":\"... No Unsupervised Loss\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly\":\"... No NP Loss\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly\":\"... Only Supervised Loss\",\n",
    "                 \"ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly\":\"... Supervised Loss, No Rescaling\",\n",
    "                      \"data sample\":\"Sample Percentage (%)\",\n",
    "                       \"lab\":\"Label Percentage (%)\"})\n",
    "\n",
    "\n",
    "out[\"lab\"] = out[\"lab\"].map(lambda x: int(str(x)[:-1]))\n",
    "\n",
    "\n",
    "out=out.rename(columns={\"models\":\"Models\",  \"lab\":\"Label Percentage (%)\"})\n",
    "\n",
    "#out = out[out.Models != \"... Supervised Loss, No Rescaling\"]\n",
    "\n",
    "#out.to_csv(\"table_ablation_har.csv\")\n",
    "\n",
    "out\n",
    "\n",
    "df = out\n",
    "df.Accuracy=df.Accuracy.astype(str).apply(' +/-'.join, axis=1)\n",
    "df=df.droplevel(1, axis=1)\n",
    "df = df.iloc[:,:-1]\n",
    "df = df.pivot_table(index=\"Label Percentage (%)\", columns=\"Models\", values=\"Accuracy\", aggfunc='first')\n",
    "\n",
    "df.to_csv(\"results/tables/table_ablation_har.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Models</th>\n",
       "      <th>... No Data Scaling</th>\n",
       "      <th>... No Entropy Minimization</th>\n",
       "      <th>... No Label Scaling</th>\n",
       "      <th>... No NP Loss</th>\n",
       "      <th>... No Negative Consistency</th>\n",
       "      <th>... No Oversampling</th>\n",
       "      <th>... No Supervised Annealing</th>\n",
       "      <th>... No Unsupervised Loss</th>\n",
       "      <th>... Pretrained</th>\n",
       "      <th>... Supervised Loss, No Rescaling</th>\n",
       "      <th>Joint UnetNP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label Percentage (%)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8823662481619726 +/-0.003628325965919103</td>\n",
       "      <td>0.8812351543942992 +/-0.0025618712030100964</td>\n",
       "      <td>0.893677185838706 +/-0.012967189672945594</td>\n",
       "      <td>0.8765976699468386 +/-0.004705948210092729</td>\n",
       "      <td>0.8849677638276213 +/-0.004450246708043437</td>\n",
       "      <td>0.8702635448478678 +/-0.0031345789979755935</td>\n",
       "      <td>0.8854202013346907 +/-0.011800295311159352</td>\n",
       "      <td>0.8716208573690759 +/-0.00463196363068507</td>\n",
       "      <td>0.8620065603438526 +/-0.01708925101078104</td>\n",
       "      <td>0.9040832485013008 +/-0.013515032337954007</td>\n",
       "      <td>0.883497341929646 +/-0.008286371635512803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9518154054971157 +/-0.0018892990712012139</td>\n",
       "      <td>0.9453681710213777 +/-0.001795555691255216</td>\n",
       "      <td>0.9533989367718583 +/-0.007567346989561189</td>\n",
       "      <td>0.9572446555819477 +/-0.0053329601783175095</td>\n",
       "      <td>0.9493269992082345 +/-0.001929501426222352</td>\n",
       "      <td>0.9347358896052483 +/-0.0016738659186686254</td>\n",
       "      <td>0.9565659993213438 +/-0.008332561355825331</td>\n",
       "      <td>0.952607171134487 +/-0.003737747232633912</td>\n",
       "      <td>0.9317950458092975 +/-0.006219987370147012</td>\n",
       "      <td>0.9639181088112205 +/-0.0049445748205845805</td>\n",
       "      <td>0.9519285148738831 +/-0.004754631728760044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Models                                        ... No Data Scaling  \\\n",
       "Label Percentage (%)                                                \n",
       "1                      0.8823662481619726 +/-0.003628325965919103   \n",
       "10                    0.9518154054971157 +/-0.0018892990712012139   \n",
       "\n",
       "Models                                ... No Entropy Minimization  \\\n",
       "Label Percentage (%)                                                \n",
       "1                     0.8812351543942992 +/-0.0025618712030100964   \n",
       "10                     0.9453681710213777 +/-0.001795555691255216   \n",
       "\n",
       "Models                                      ... No Label Scaling  \\\n",
       "Label Percentage (%)                                               \n",
       "1                      0.893677185838706 +/-0.012967189672945594   \n",
       "10                    0.9533989367718583 +/-0.007567346989561189   \n",
       "\n",
       "Models                                             ... No NP Loss  \\\n",
       "Label Percentage (%)                                                \n",
       "1                      0.8765976699468386 +/-0.004705948210092729   \n",
       "10                    0.9572446555819477 +/-0.0053329601783175095   \n",
       "\n",
       "Models                               ... No Negative Consistency  \\\n",
       "Label Percentage (%)                                               \n",
       "1                     0.8849677638276213 +/-0.004450246708043437   \n",
       "10                    0.9493269992082345 +/-0.001929501426222352   \n",
       "\n",
       "Models                                        ... No Oversampling  \\\n",
       "Label Percentage (%)                                                \n",
       "1                     0.8702635448478678 +/-0.0031345789979755935   \n",
       "10                    0.9347358896052483 +/-0.0016738659186686254   \n",
       "\n",
       "Models                               ... No Supervised Annealing  \\\n",
       "Label Percentage (%)                                               \n",
       "1                     0.8854202013346907 +/-0.011800295311159352   \n",
       "10                    0.9565659993213438 +/-0.008332561355825331   \n",
       "\n",
       "Models                                 ... No Unsupervised Loss  \\\n",
       "Label Percentage (%)                                              \n",
       "1                     0.8716208573690759 +/-0.00463196363068507   \n",
       "10                    0.952607171134487 +/-0.003737747232633912   \n",
       "\n",
       "Models                                            ... Pretrained  \\\n",
       "Label Percentage (%)                                               \n",
       "1                      0.8620065603438526 +/-0.01708925101078104   \n",
       "10                    0.9317950458092975 +/-0.006219987370147012   \n",
       "\n",
       "Models                          ... Supervised Loss, No Rescaling  \\\n",
       "Label Percentage (%)                                                \n",
       "1                      0.9040832485013008 +/-0.013515032337954007   \n",
       "10                    0.9639181088112205 +/-0.0049445748205845805   \n",
       "\n",
       "Models                                              Joint UnetNP  \n",
       "Label Percentage (%)                                              \n",
       "1                      0.883497341929646 +/-0.008286371635512803  \n",
       "10                    0.9519285148738831 +/-0.004754631728760044  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=\"Label Percentage (%)\", columns=\"Models\", values=\"Accuracy\", aggfunc='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Label Percentage (%)</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Accuracy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Convergence Epoch</th>\n",
       "      <th colspan=\"2\" halign=\"left\">99% Convergence Epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>... Pretrained</td>\n",
       "      <td>1</td>\n",
       "      <td>0.862007</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.785939</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.081666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>... Pretrained</td>\n",
       "      <td>10</td>\n",
       "      <td>0.931795</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>3.785939</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joint UnetNP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883497</td>\n",
       "      <td>0.008286</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joint UnetNP</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951929</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.511885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>... No Oversampling</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870264</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>6.806859</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>... No Oversampling</td>\n",
       "      <td>10</td>\n",
       "      <td>0.934736</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>6.506407</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>... No Data Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882366</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>... No Data Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951815</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>... No Entropy Minimization</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881235</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>12.701706</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>... No Entropy Minimization</td>\n",
       "      <td>10</td>\n",
       "      <td>0.945368</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>... No Label Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893677</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>1.154701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>... No Label Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953399</td>\n",
       "      <td>0.007567</td>\n",
       "      <td>19.333333</td>\n",
       "      <td>8.504901</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.588457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>... No Supervised Annealing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885420</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>... No Supervised Annealing</td>\n",
       "      <td>10</td>\n",
       "      <td>0.956566</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.928203</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.309401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>... No Negative Consistency</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884968</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>8.144528</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>... No Negative Consistency</td>\n",
       "      <td>10</td>\n",
       "      <td>0.949327</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2.516611</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.527525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>... No Unsupervised Loss</td>\n",
       "      <td>1</td>\n",
       "      <td>0.871621</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.309401</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.309401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>... No Unsupervised Loss</td>\n",
       "      <td>10</td>\n",
       "      <td>0.952607</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>6.429101</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.154701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>... No NP Loss</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876598</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>11.590226</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.527525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>... No NP Loss</td>\n",
       "      <td>10</td>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>4.725816</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>3.214550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>... Supervised Loss, No Rescaling</td>\n",
       "      <td>1</td>\n",
       "      <td>0.904083</td>\n",
       "      <td>0.013515</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>... Supervised Loss, No Rescaling</td>\n",
       "      <td>10</td>\n",
       "      <td>0.963918</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>5.859465</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>2.081666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Models Label Percentage (%)  Accuracy  \\\n",
       "                                                                mean   \n",
       "0                      ... Pretrained                    1  0.862007   \n",
       "1                      ... Pretrained                   10  0.931795   \n",
       "2                        Joint UnetNP                    1  0.883497   \n",
       "3                        Joint UnetNP                   10  0.951929   \n",
       "4                 ... No Oversampling                    1  0.870264   \n",
       "5                 ... No Oversampling                   10  0.934736   \n",
       "6                 ... No Data Scaling                    1  0.882366   \n",
       "7                 ... No Data Scaling                   10  0.951815   \n",
       "8         ... No Entropy Minimization                    1  0.881235   \n",
       "9         ... No Entropy Minimization                   10  0.945368   \n",
       "10               ... No Label Scaling                    1  0.893677   \n",
       "11               ... No Label Scaling                   10  0.953399   \n",
       "12        ... No Supervised Annealing                    1  0.885420   \n",
       "13        ... No Supervised Annealing                   10  0.956566   \n",
       "14        ... No Negative Consistency                    1  0.884968   \n",
       "15        ... No Negative Consistency                   10  0.949327   \n",
       "16           ... No Unsupervised Loss                    1  0.871621   \n",
       "17           ... No Unsupervised Loss                   10  0.952607   \n",
       "18                     ... No NP Loss                    1  0.876598   \n",
       "19                     ... No NP Loss                   10  0.957245   \n",
       "20  ... Supervised Loss, No Rescaling                    1  0.904083   \n",
       "21  ... Supervised Loss, No Rescaling                   10  0.963918   \n",
       "\n",
       "             Convergence Epoch            99% Convergence Epoch             \n",
       "         std              mean        std                  mean        std  \n",
       "0   0.017089          3.666667   3.785939              2.666667   2.081666  \n",
       "1   0.006220          8.666667   3.785939              1.333333   0.577350  \n",
       "2   0.008286          5.333333   3.214550              2.666667   0.577350  \n",
       "3   0.004755          8.000000   7.000000              4.333333   3.511885  \n",
       "4   0.003135         17.333333   6.806859              9.000000   5.000000  \n",
       "5   0.001674         13.333333   6.506407              1.666667   0.577350  \n",
       "6   0.003628          2.666667   0.577350              2.000000   0.000000  \n",
       "7   0.001889          8.666667   2.081666              2.000000   0.000000  \n",
       "8   0.002562          8.333333  12.701706              2.000000   1.732051  \n",
       "9   0.001796          3.666667   1.527525              3.000000   1.000000  \n",
       "10  0.012967         13.000000   0.000000             10.333333   1.154701  \n",
       "11  0.007567         19.333333   8.504901             11.000000  15.588457  \n",
       "12  0.011800          3.666667   1.527525              3.000000   1.000000  \n",
       "13  0.008333          6.000000   6.928203              3.333333   2.309401  \n",
       "14  0.004450          6.666667   8.144528              2.000000   1.000000  \n",
       "15  0.001930          4.333333   2.516611              3.666667   1.527525  \n",
       "16  0.004632          2.333333   2.309401              2.333333   2.309401  \n",
       "17  0.003738         20.333333   6.429101              2.333333   1.154701  \n",
       "18  0.004706          9.666667  11.590226              3.666667   1.527525  \n",
       "19  0.005333         10.666667   4.725816              5.666667   3.214550  \n",
       "20  0.013515          3.333333   1.527525              2.333333   0.577350  \n",
       "21  0.004945         10.333333   5.859465              3.666667   2.081666  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "out = pd.DataFrame({k:[v.history[-1][\"valid_acc\"], len(v.history), get_percentile_converge_epoch(v.history)] \n",
    "                 for k,v in data_trainers.items()}, index=[\"Accuracy\", \"Convergence Epoch\", \"99% Convergence Epoch\"]\n",
    "                  ).T.reset_index()#name=\"accuracy\")\n",
    "splitted = out[\"index\"].str.split(\"/\", expand = True)\n",
    "out[\"meta\"] = splitted[0]\n",
    "out[\"models\"] = splitted[1]\n",
    "\n",
    "splitted2 = out[\"meta\"].str.split(\"_run\", expand = True)\n",
    "out[\"meta\"] = splitted2[0]\n",
    "out[\"run\"] = splitted2[1]\n",
    "\n",
    "splitted3 = out[\"meta\"].str.split(\"_lab\", expand = True)\n",
    "out[\"data sample\"] = splitted3[0].str.split(\"har\", expand = True)[1]\n",
    "out[\"lab\"] = splitted3[1]\n",
    "\n",
    "\n",
    "out.drop(columns =[\"index\"], inplace = True) \n",
    "\n",
    "out = out.groupby([\"models\", \"lab\"]).agg([\"mean\", \"std\"])\n",
    "\n",
    "out.reset_index(drop=False, inplace=True)\n",
    "\n",
    "out=out.replace({\"ssl_classifier_gnp_large_shared_bottleneck_finetune\":\"Joint UnetNP\", \n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck\":\"... Pretrained\", \n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_noaug\":\"... No Oversampling\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_noelemscale\":\"... No Data Scaling\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_noent\":\"... No Entropy Minimization\",\n",
    "                     \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nolabscale\": \"... No Label Scaling\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nolambda\":\"... No Supervised Annealing\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nonegcons\":\"... No Negative Consistency\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_nounsup\":\"... No Unsupervised Loss\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_sslonly\":\"... No NP Loss\",\n",
    "                      \"ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly\":\"... Only Supervised Loss\",\n",
    "                 \"ssl_classifier_gnp_large_shared_bottleneck_finetune_suponly\":\"... Supervised Loss, No Rescaling\",\n",
    "                      \"data sample\":\"Sample Percentage (%)\",\n",
    "                       \"lab\":\"Label Percentage (%)\"})\n",
    "\n",
    "\n",
    "out[\"lab\"] = out[\"lab\"].map(lambda x: int(str(x)[:-1]))\n",
    "\n",
    "\n",
    "out=out.rename(columns={\"models\":\"Models\",  \"lab\":\"Label Percentage (%)\"})\n",
    "\n",
    "\n",
    "\n",
    "out.to_csv(\"results/tables/table_ablation_har_nopivot.csv\")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
