{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition - GNP Featurized\n",
    "\n",
    "Last Update : 21 July 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 8\n",
    "# Nota Bene : notebooks don't deallocate GPU memory\n",
    "IS_FORCE_CPU = False # can also be set in the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/master\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
       ".prompt display:none;}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# CENTER PLOTS\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\" <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
    ".prompt display:none;}  </style>\"\"\"))\n",
    "\n",
    "import os\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skssl.transformers.neuralproc.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "from utils.data.tsdata import get_timeseries_dataset, SparseMultiTimeSeriesDataset\n",
    "\n",
    "\n",
    "get_cntxt_trgt_test = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.1, max_n_indcs=0.5),\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt_feat = CntxtTrgtGetter(contexts_getter=get_all_indcs,\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "\n",
    "get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                 targets_getter=GetRandomIndcs(min_n_indcs=0.5, max_n_indcs=0.99),\n",
    "                                 is_add_cntxts_to_trgts=False)  # don't context points to tagrtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_train = get_timeseries_dataset(\"har\")(split=\"train\")\n",
    "data_test = get_timeseries_dataset(\"har\")(split=\"test\")\n",
    "data_both = get_timeseries_dataset(\"har\")(split=\"both\")\n",
    "\n",
    "def cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=False):\n",
    "    def mycollate(batch):\n",
    "        min_length = min([v.size(0) for b in batch for k,v in b[0].items() if \"X\" in k])\n",
    "        # chose first min_legth of each (assumes that randomized)\n",
    "        \n",
    "        batch = [({k:v[:min_length, ...] for k,v in b[0].items()}, b[1]) for b in batch]        \n",
    "        collated = torch.utils.data.dataloader.default_collate(batch)\n",
    "        \n",
    "        X = collated[0][\"X\"]\n",
    "        y = collated[0][\"y\"]\n",
    "        \n",
    "        if is_repeat_batch:\n",
    "            \n",
    "            X = torch.cat([X,X], dim=0)\n",
    "            y = torch.cat([y,y], dim=0)\n",
    "        \n",
    "        collated[0][\"X\"], collated[0][\"y\"], collated[0][\"X_trgt\"], collated[0][\"y_trgt\"] = get_cntxt_trgt(X, y)\n",
    "        \n",
    "        return collated\n",
    "    return mycollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DIM = 1  # 1D spatial input (although actually 2 but the first is for sparse channels)\n",
    "Y_DIM = data_train.data.shape[-1] # multiple channels\n",
    "N_TARGETS = len(np.unique(data_train.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from skssl.transformers import GlobalNeuralProcess, NeuralProcessLoss, AttentiveNeuralProcess\n",
    "from skssl.utils.helpers import rescale_range\n",
    "from skssl.predefined import UnetCNN, CNN, MLP, SparseSetConv, SetConv, MlpRBF, GaussianRBF, BatchSparseSetConv\n",
    "from skssl.transformers.neuralproc.datasplit import precomputed_cntxt_trgt_split\n",
    "from utils.helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "models = {}\n",
    "\n",
    "unet = partial(UnetCNN,\n",
    "               Conv=torch.nn.Conv1d,\n",
    "               Pool=torch.nn.MaxPool1d,\n",
    "               upsample_mode=\"linear\",\n",
    "               n_layers=18,\n",
    "               is_double_conv=True,\n",
    "               is_depth_separable=True,\n",
    "               Normalization=torch.nn.BatchNorm1d,\n",
    "               is_chan_last=True,\n",
    "               bottleneck=None,\n",
    "               kernel_size=7,\n",
    "               max_nchannels=256)\n",
    "\n",
    "kwargs = dict(x_dim=X_DIM, \n",
    "              y_dim=Y_DIM,\n",
    "              min_std=5e-3,\n",
    "                n_tmp_queries=128,\n",
    "                r_dim=64,\n",
    "              keys_to_tmp_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              TmpSelfAttn=unet,\n",
    "              tmp_to_queries_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              is_skip_tmp=False,\n",
    "              is_use_x=False,\n",
    "              get_cntxt_trgt=precomputed_cntxt_trgt_split,\n",
    "              is_encode_xy=False)\n",
    "\n",
    "models[\"transformer_gnp_large\"] = partial(GlobalNeuralProcess, **kwargs)\n",
    "\n",
    "kwargs_bis = deepcopy(kwargs)\n",
    "kwargs_bis[\"TmpSelfAttn\"] = partial(unet, n_layers=14, bottleneck=True)\n",
    "\n",
    "models[\"transformer_gnp_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_gnp_large - N Param: 1006936\n",
      "transformer_gnp_bottleneck - N Param: 665944\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "for k,v in models.items():\n",
    "    print(k, \"- N Param:\", count_parameters(v()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntbks_helpers import train_models_\n",
    "from skorch.dataset import CVSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100 \n",
    "BATCH_SIZE = 64\n",
    "IS_RETRAIN = False # if false load precomputed\n",
    "chckpnt_dirname=\"results/challenge/har/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trainers = {}\n",
    "data_percentages =  [0.05, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "datas = {p:get_timeseries_dataset(\"har\")(split=\"both\", data_perc=p, is_fill_mean=False)\n",
    "         for p in data_percentages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 5%har/transformer_gnp_large ---\n",
      "\n",
      "5%har/transformer_gnp_large best epoch: 63 val_loss: -1.2570536011631048\n",
      "\n",
      "--- Loading 5%har/transformer_gnp_bottleneck ---\n",
      "\n",
      "5%har/transformer_gnp_bottleneck best epoch: 64 val_loss: -1.3726283962286792\n",
      "\n",
      "--- Loading 10%har/transformer_gnp_large ---\n",
      "\n",
      "10%har/transformer_gnp_large best epoch: 48 val_loss: -1.1501760788334225\n",
      "\n",
      "--- Loading 10%har/transformer_gnp_bottleneck ---\n",
      "\n",
      "10%har/transformer_gnp_bottleneck best epoch: 39 val_loss: -0.18719741117607042\n",
      "\n",
      "--- Loading 30%har/transformer_gnp_large ---\n",
      "\n",
      "30%har/transformer_gnp_large best epoch: 66 val_loss: -3.663724695536697\n",
      "\n",
      "--- Loading 30%har/transformer_gnp_bottleneck ---\n",
      "\n",
      "30%har/transformer_gnp_bottleneck best epoch: 54 val_loss: -3.0089645237598606\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large ---\n",
      "\n",
      "50%har/transformer_gnp_large best epoch: 65 val_loss: -4.738986269247184\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_bottleneck best epoch: 40 val_loss: -2.2714064218465566\n",
      "\n",
      "--- Loading 70%har/transformer_gnp_large ---\n",
      "\n",
      "70%har/transformer_gnp_large best epoch: 46 val_loss: -4.399586157168\n",
      "\n",
      "--- Loading 70%har/transformer_gnp_bottleneck ---\n",
      "\n",
      "70%har/transformer_gnp_bottleneck best epoch: 53 val_loss: -3.7146270080677515\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large ---\n",
      "\n",
      "100%har/transformer_gnp_large best epoch: 70 val_loss: -5.671105713520236\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_bottleneck best epoch: 64 val_loss: -4.973251605959772\n"
     ]
    }
   ],
   "source": [
    "for p,data in datas.items():\n",
    "    data_trainers.update(train_models_({\"{}%har\".format(int(p*100)): data}, \n",
    "                              models, \n",
    "                              NeuralProcessLoss,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              train_split=CVSplit(0.05),\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_test),\n",
    "                              mode=\"transformer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_data(chckpnt_dirname, data_trainers, datas):\n",
    "    for k, trainer in data_trainers.items():\n",
    "        perc = float(k.split(\"%\")[0]) / 100\n",
    "        model_name = k.split(\"/\")[1]\n",
    "        data = datas[perc]\n",
    "        trainer.set_params(iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                           iterator_valid__shuffle=False) # make sure not shuffling because only transforming X\n",
    "        transformed_data = trainer.transform(data)\n",
    "        np.save(chckpnt_dirname+k+\"/transformed_data.npy\", transformed_data, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_transformed_data(chckpnt_dirname, data_trainers, datas)\n",
    "# saving all transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from skssl.predefined import RNN, MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, feat_size, y_dim, output_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.out = MLP(feat_size+y_dim*4, output_size, **kwargs)\n",
    "\n",
    "    def forward(self, X, X_feat, y=None):\n",
    "\n",
    "        if y is not None:\n",
    "            # is there's y then that's the actual input and X is time\n",
    "            X = y\n",
    "\n",
    "        outputs = self.out(torch.cat([X_feat, X.mean(-2), X.max(-2)[0], X.min(-2)[0], X.var(-2)], dim=-1))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict(mlp_hand = partial(FeatureMLP, feat_size=256, y_dim=Y_DIM, output_size=N_TARGETS, hidden_size=128, \n",
    "                dropout=0.5, n_hidden_layers=3, is_res=True),\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_hand - N Param: 71302\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "for k,v in models.items():\n",
    "    print(k, \"- N Param:\", count_parameters(v()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_on_featurized(X_train, X_test, Y_train, Y_test):\n",
    "    mlp = MLPClassifier(solver=\"lbfgs\", hidden_layer_sizes=(128,))\n",
    "    mlp = SVC()\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    print(mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 5%har/transformer_gnp_large/feat_mlp_hand ---\n",
      "\n",
      "5%har/transformer_gnp_large/feat_mlp_hand best epoch: 25 val_loss: 0.3994055680594365\n",
      "\n",
      "--- Loading 5%har/transformer_gnp_bottleneck/feat_mlp_hand ---\n",
      "\n",
      "5%har/transformer_gnp_bottleneck/feat_mlp_hand best epoch: 27 val_loss: 0.40084968634205104\n",
      "\n",
      "--- Loading 10%har/transformer_gnp_large/feat_mlp_hand ---\n",
      "\n",
      "10%har/transformer_gnp_large/feat_mlp_hand best epoch: 17 val_loss: 0.40527173914179626\n",
      "\n",
      "--- Loading 10%har/transformer_gnp_bottleneck/feat_mlp_hand ---\n",
      "\n",
      "10%har/transformer_gnp_bottleneck/feat_mlp_hand best epoch: 12 val_loss: 0.40299248517388847\n",
      "\n",
      "--- Loading 30%har/transformer_gnp_large/feat_mlp_hand ---\n",
      "\n",
      "30%har/transformer_gnp_large/feat_mlp_hand best epoch: 24 val_loss: 0.2857550858721595\n",
      "\n",
      "--- Loading 30%har/transformer_gnp_bottleneck/feat_mlp_hand ---\n",
      "\n",
      "30%har/transformer_gnp_bottleneck/feat_mlp_hand best epoch: 25 val_loss: 0.26946120105195864\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large/feat_mlp_hand ---\n",
      "\n",
      "50%har/transformer_gnp_large/feat_mlp_hand best epoch: 3 val_loss: 0.20970985878885934\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_bottleneck/feat_mlp_hand ---\n",
      "\n",
      "50%har/transformer_gnp_bottleneck/feat_mlp_hand best epoch: 23 val_loss: 0.2706818469007783\n",
      "\n",
      "--- Loading 70%har/transformer_gnp_large/feat_mlp_hand ---\n",
      "\n",
      "70%har/transformer_gnp_large/feat_mlp_hand best epoch: 14 val_loss: 0.2614200467937147\n",
      "\n",
      "--- Loading 70%har/transformer_gnp_bottleneck/feat_mlp_hand ---\n",
      "\n",
      "70%har/transformer_gnp_bottleneck/feat_mlp_hand best epoch: 16 val_loss: 0.22425987731038383\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large/feat_mlp_hand ---\n",
      "\n",
      "100%har/transformer_gnp_large/feat_mlp_hand best epoch: 17 val_loss: 0.16193591079996658\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_bottleneck/feat_mlp_hand ---\n",
      "\n",
      "100%har/transformer_gnp_bottleneck/feat_mlp_hand best epoch: 15 val_loss: 0.20337932628417396\n"
     ]
    }
   ],
   "source": [
    "data_trainers_feat = {}\n",
    "\n",
    "for k, trainer in data_trainers.items():\n",
    "    perc = float(k.split(\"%\")[0]) / 100\n",
    "    n_train = datas[perc].n_train\n",
    "    model_name = k.split(\"/\")[1]\n",
    "    y = datas[perc].targets\n",
    "    transformed_data = np.load(chckpnt_dirname+k+\"/transformed_data.npy\", allow_pickle=False)\n",
    "    X_train, X_test, Y_train, Y_test = transformed_data[:n_train], transformed_data[n_train:], y[:n_train].squeeze(), y[n_train:].squeeze()\n",
    "    #train_eval_on_featurized(X_train, X_test, Y_train, Y_test)\n",
    "    \n",
    "                                                \n",
    "    data_trainers_feat.update(train_models_({\"{}%har\".format(int(perc*100)): (Dataset(X={\"X_feat\":X_train.astype(np.float32), \n",
    "                                                                   \"X\":datas[perc].data[:n_train].astype(np.float32)}, \n",
    "                                                                y=torch.from_numpy(Y_train).long()), \n",
    "                                                        Dataset(X={\"X_feat\":X_test.astype(np.float32), \n",
    "                                                                   \"X\":datas[perc].data[n_train:].astype(np.float32)}, \n",
    "                                                                y=torch.from_numpy(Y_test).long()))}, \n",
    "                                               {model_name +\"/feat_\"+ name: partial(m, feat_size=X_train.shape[-1]) for name,m in models.items()}, \n",
    "                                                 chckpnt_dirname=chckpnt_dirname,\n",
    "                                                  max_epochs=N_EPOCHS,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                 is_retrain=False))\n",
    "    \n",
    "    \n",
    "# 100%: MLP 0.8235 (no feat)\n",
    "# \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5%har/transformer_gnp_large/feat_mlp_hand epoch: 25 val_loss: 0.3994055680594365 val_acc: 0.845945028842891\n",
      "5%har/transformer_gnp_bottleneck/feat_mlp_hand epoch: 27 val_loss: 0.40084968634205104 val_acc: 0.8500169664065151\n",
      "10%har/transformer_gnp_large/feat_mlp_hand epoch: 17 val_loss: 0.40527173914179626 val_acc: 0.833050559891415\n",
      "10%har/transformer_gnp_bottleneck/feat_mlp_hand epoch: 12 val_loss: 0.40299248517388847 val_acc: 0.842212419409569\n",
      "30%har/transformer_gnp_large/feat_mlp_hand epoch: 24 val_loss: 0.2857550858721595 val_acc: 0.9131319986426875\n",
      "30%har/transformer_gnp_bottleneck/feat_mlp_hand epoch: 25 val_loss: 0.26946120105195864 val_acc: 0.9117746861214795\n",
      "50%har/transformer_gnp_large/feat_mlp_hand epoch: 3 val_loss: 0.20970985878885934 val_acc: 0.9097387173396675\n",
      "50%har/transformer_gnp_bottleneck/feat_mlp_hand epoch: 23 val_loss: 0.2706818469007783 val_acc: 0.9161859518154055\n",
      "70%har/transformer_gnp_large/feat_mlp_hand epoch: 14 val_loss: 0.2614200467937147 val_acc: 0.9070240922972514\n",
      "70%har/transformer_gnp_bottleneck/feat_mlp_hand epoch: 16 val_loss: 0.22425987731038383 val_acc: 0.9172039362063115\n",
      "100%har/transformer_gnp_large/feat_mlp_hand epoch: 17 val_loss: 0.16193591079996658 val_acc: 0.9487614523243977\n",
      "100%har/transformer_gnp_bottleneck/feat_mlp_hand epoch: 15 val_loss: 0.20337932628417396 val_acc: 0.9199185612487275\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers_feat\n",
    ".items():\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trainers_feat = {}\n",
    "\n",
    "\n",
    "perc = 1#float(k.split(\"%\")[0]) / 100\n",
    "n_train = datas[perc].n_train\n",
    "model_name = k.split(\"/\")[1]\n",
    "y = datas[perc].targets\n",
    "transformed_data = np.load(chckpnt_dirname+k+\"/transformed_data.npy\", allow_pickle=False)\n",
    "X_train, X_test, Y_train, Y_test = transformed_data[:n_train], transformed_data[n_train:], y[:n_train].squeeze(), y[n_train:].squeeze()\n",
    "#train_eval_on_featurized(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "\n",
    "data_trainers_feat.update(train_models_({\"{}%har\".format(int(perc*100)): (Dataset(X={\"X_feat\":X_train.astype(np.float32), \n",
    "                                                               \"X\":datas[perc].data[:n_train].astype(np.float32)}, \n",
    "                                                            y=torch.from_numpy(Y_train).long()), \n",
    "                                                    Dataset(X={\"X_feat\":X_test.astype(np.float32), \n",
    "                                                               \"X\":datas[perc].data[n_train:].astype(np.float32)}, \n",
    "                                                            y=torch.from_numpy(Y_test).long()))}, \n",
    "                                           {model_name +\"/feat_\"+ name: partial(m, feat_size=X_train.shape[-1]) for name,m in models.items()}, \n",
    "                                             chckpnt_dirname=chckpnt_dirname,\n",
    "                                              max_epochs=N_EPOCHS,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                             is_retrain=False))\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transformer_gnp_large', 'transformer_gnp_bottleneck'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data.helpers import make_ssl_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 1#float(k.split(\"%\")[0]) / 100\n",
    "transformed_data = np.load(chckpnt_dirname+\"{}%har/\".format(int(perc*100))+'transformer_gnp_large'+\"/transformed_data.npy\", allow_pickle=False)\n",
    "data = deepcopy(datas[1])\n",
    "data.train = transformed_data\n",
    "data_train = make_ssl_dataset_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-56ab9994b84f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'perc' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trainers = {}\n",
    "data_perc = 1\n",
    "\n",
    "for label_perc in label_percentages:\n",
    "    data_train, _, data_test = get_train_dev_test_ssl(\"har\", n_labels=label_perc, data_perc=data_perc, dev_size=0)\n",
    "    \n",
    "    data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(data_perc*100), int(label_perc*100)): (data_train, data_test)}, \n",
    "                         models, \n",
    "                         chckpnt_dirname=chckpnt_dirname,\n",
    "                          max_epochs=N_EPOCHS,\n",
    "                          batch_size=64,\n",
    "                         is_retrain=True, \n",
    "                         iterator_train=get_supervised_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = 3000\n",
    "data = data_train\n",
    "n_targets = 60\n",
    "\n",
    "idcs = torch.randperm(128)[:n_targets]\n",
    "X_cntxt = data[eg][0][\"X\"][idcs].unsqueeze(0)\n",
    "Y_cntxt = data[eg][0][\"y\"][idcs].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntbks_viz import plot_posterior_predefined_cntxt\n",
    "plot_posterior_predefined_cntxt(data_trainers[3]['50%har/transformer_gnp_large'].module_, \n",
    "                                true_func=(data[eg][0][\"X\"].unsqueeze(0), data[eg][0][\"y\"].unsqueeze(0)),\n",
    "                               X_cntxt=X_cntxt, \n",
    "                               Y_cntxt=Y_cntxt,\n",
    "                               n_trgt=128,\n",
    "                               n_samples=1,\n",
    "                               is_plot_std=True,\n",
    "                               train_min_max=(-1,1),\n",
    "                               title=\"Posterior Samples Conditioned on {} Context Points \".format(10),\n",
    "                                       y_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "params = {\n",
    "    'module__x_dim': [X_DIM],\n",
    "    'module__y_dim': [Y_DIM],\n",
    "    'module__r_dim': [8,16,32,64],\n",
    "    #'module__keys_to_tmp_attn__RadialBasisFunc': [MlpRBF, GaussianRBF],\n",
    "    #'module__TmpSelfAttn__is_depth_separable': Categorical([True,False]),\n",
    "}\n",
    "\n",
    "def scorer(model, X, y=None):\n",
    "    score = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    dataset = model.get_dataset(X, y)\n",
    "\n",
    "    # use th eloss as metric (i.e. return log likelihood)\n",
    "    model.criterion_.is_use_as_metric = True\n",
    "    \n",
    "    valid_batch_count = 0\n",
    "    for Xi, yi in model.get_iterator(dataset, training=False):\n",
    "        n_steps += 1\n",
    "        yi_res = yi \n",
    "        step = model.validation_step(Xi, yi)\n",
    "        score += step[\"loss\"].item()\n",
    "        \n",
    "    score /= n_steps\n",
    "    \n",
    "    return score\n",
    "\n",
    "gs = BayesSearchCV(model, \n",
    "                   params, \n",
    "                   refit=False, \n",
    "                   cv=3, \n",
    "                   scoring=scorer)\n",
    "\n",
    "gs.fit(skorch.helper.SliceDataset(data_train))\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
