{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition - SSL JOINT Consistency\n",
    "\n",
    "Last Update : 24 July 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 8\n",
    "# Nota Bene : notebooks don't deallocate GPU memory\n",
    "IS_FORCE_CPU = False # can also be set in the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/conv\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
       ".prompt display:none;}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# CENTER PLOTS\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\" <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
    ".prompt display:none;}  </style>\"\"\"))\n",
    "\n",
    "import os\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skssl.transformers.neuralproc.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "from utils.data.tsdata import get_timeseries_dataset, SparseMultiTimeSeriesDataset\n",
    "\n",
    "get_cntxt_trgt_test = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.1, max_n_indcs=0.5),\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt_feat = CntxtTrgtGetter(contexts_getter=get_all_indcs,\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                 targets_getter=GetRandomIndcs(min_n_indcs=0.5, max_n_indcs=0.99),\n",
    "                                 is_add_cntxts_to_trgts=False)  # don't context points to tagrtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_both = get_timeseries_dataset(\"har\")(split=\"both\")\n",
    "\n",
    "def cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=False):\n",
    "    def mycollate(batch):\n",
    "        min_length = min([v.size(0) for b in batch for k,v in b[0].items() if \"X\" in k])\n",
    "        # chose first min_legth of each (assumes that randomized)\n",
    "        \n",
    "        batch = [({k:v[:min_length, ...] for k,v in b[0].items()}, b[1]) for b in batch]        \n",
    "        collated = torch.utils.data.dataloader.default_collate(batch)\n",
    "        \n",
    "        X = collated[0][\"X\"]\n",
    "        y = collated[0][\"y\"]\n",
    "        \n",
    "        if is_repeat_batch:\n",
    "            \n",
    "            X = torch.cat([X,X], dim=0)\n",
    "            y = torch.cat([y,y], dim=0)\n",
    "            collated[1] = torch.cat([collated[1], collated[1]], dim=0) # targets\n",
    "        \n",
    "        collated[0][\"X\"], collated[0][\"y\"], collated[0][\"X_trgt\"], collated[0][\"y_trgt\"] = get_cntxt_trgt(X, y)\n",
    "        \n",
    "        return collated\n",
    "    return mycollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DIM = 1  # 1D spatial input (although actually 2 but the first is for sparse channels)\n",
    "Y_DIM = data_both.data.shape[-1] # multiple channels\n",
    "N_TARGETS = len(np.unique(data_both.targets))\n",
    "\n",
    "sampling_percentages = [0.05, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "label_percentages = [N_TARGETS, N_TARGETS*2, 0.01, 0.05, 0.1, 0.3, 0.5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from skssl.transformers import GlobalNeuralProcess, NeuralProcessLoss, AttentiveNeuralProcess\n",
    "from skssl.utils.helpers import rescale_range\n",
    "from skssl.predefined import UnetCNN, CNN, MLP, SparseSetConv, SetConv, MlpRBF, GaussianRBF, BatchSparseSetConv\n",
    "from skssl.transformers.neuralproc.datasplit import precomputed_cntxt_trgt_split\n",
    "from utils.helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "models = {}\n",
    "\n",
    "unet = partial(UnetCNN,\n",
    "               Conv=torch.nn.Conv1d,\n",
    "               Pool=torch.nn.MaxPool1d,\n",
    "               upsample_mode=\"linear\",\n",
    "               n_layers=18,\n",
    "               is_double_conv=True,\n",
    "               is_depth_separable=True,\n",
    "               Normalization=torch.nn.BatchNorm1d,\n",
    "               is_chan_last=True,\n",
    "               bottleneck=None,\n",
    "               kernel_size=7,\n",
    "               max_nchannels=256,\n",
    "              is_force_same_bottleneck=True,\n",
    "               _is_summary=True,\n",
    "              )\n",
    "\n",
    "kwargs = dict(x_dim=X_DIM, \n",
    "              y_dim=Y_DIM,\n",
    "              min_std=5e-3,\n",
    "                n_tmp_queries=128,\n",
    "                r_dim=64,\n",
    "              keys_to_tmp_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              TmpSelfAttn=unet,\n",
    "              tmp_to_queries_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              is_skip_tmp=False,\n",
    "              is_use_x=False,\n",
    "              get_cntxt_trgt=precomputed_cntxt_trgt_split,\n",
    "              is_encode_xy=False,\n",
    "             Classifier=partial(MLP, input_size=256+Y_DIM*4, output_size=N_TARGETS, \n",
    "                                dropout=0.5, hidden_size=128, n_hidden_layers=3, is_res=True))\n",
    "\n",
    "models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)\n",
    "\n",
    "kwargs_bis = deepcopy(kwargs)\n",
    "kwargs_bis[\"Classifier\"] = None\n",
    "\n",
    "models[\"transformer_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssl_classifier_gnp_large_shared_bottleneck - N Param: 1078238\n",
      "transformer_gnp_large_shared_bottleneck - N Param: 1006936\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "for k,v in models.items():\n",
    "    print(k, \"- N Param:\", count_parameters(v()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_(models, sampling_percentages):\n",
    "    # ALREADY INITALIZE TO BE ABLE TO LOAD\n",
    "    models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)()\n",
    "\n",
    "    kwargs_bis = deepcopy(kwargs)\n",
    "    kwargs_bis[\"Classifier\"] = None\n",
    "\n",
    "    models[\"transformer_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)()\n",
    "\n",
    "    # load all transformers\n",
    "    loaded_models = {}\n",
    "    for sampling_perc in sampling_percentages:\n",
    "        for k, m in models.items():\n",
    "            if \"transformer\" not in k:\n",
    "                continue\n",
    "\n",
    "            out = train_models_({\"{}%har\".format(int(sampling_perc*100)): \n",
    "                                                (None, None)}, \n",
    "                                  {k :m },\n",
    "                                   chckpnt_dirname=chckpnt_dirname,\n",
    "                                   is_retrain=False)\n",
    "\n",
    "            pretrained_model = out[list(out.keys())[0]].module_\n",
    "            model_dict = models[k.replace(\"transformer\", \"ssl_classifier\")].state_dict()\n",
    "            model_dict.update(pretrained_model.state_dict())\n",
    "            models[k.replace(\"transformer\", \"ssl_classifier\")].load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntbks_helpers import train_models_\n",
    "from skorch.dataset import CVSplit\n",
    "from utils.data.ssldata import get_train_dev_test_ssl\n",
    "\n",
    "N_EPOCHS = 100 \n",
    "BATCH_SIZE = 32\n",
    "IS_RETRAIN = False # if false load precomputed\n",
    "chckpnt_dirname=\"results/challenge/har/\"\n",
    "\n",
    "from skssl.utils.helpers import HyperparameterInterpolator\n",
    "\n",
    "n_steps_per_epoch = len(data_both)//BATCH_SIZE\n",
    "get_lambda_clf=HyperparameterInterpolator(1e-5, 10, N_EPOCHS*n_steps_per_epoch, \n",
    "                              start_step=n_steps_per_epoch*10, mode=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck best epoch: 21 val_loss: 21.35151945585957\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [1]:\n",
    "    for label_perc in [0.1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total # label perc is lower ebcause cocnat to test\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck epoch: 21 val_loss: 21.35151945585957 val_acc: 0.9419748897183576\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items():\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Neg Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading 100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_no_negcons ---\n",
      "\n",
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_no_negcons best epoch: 34 val_loss: 14.35759756789193\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [1]:\n",
    "    for label_perc in [.1]:\n",
    "        load_pretrained_(models, [sampling_perc])\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune_no_negcons\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                is_neg_consistency=False,\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_no_negcons epoch: 34 val_loss: 14.35759756789193 val_acc: 0.9677638276213099\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading 100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 47 val_loss: 13.080543520655274\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [1]:\n",
    "    for label_perc in [.1]:\n",
    "        load_pretrained_(models, [sampling_perc])\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune_sup\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"supervised\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 47 val_loss: 13.080543520655274 val_acc: 0.9738717339667459\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without N Max Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading 100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_no_cntxt_scaling ---\n",
      "\n",
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_no_cntxt_scaling best epoch: 10 val_loss: 20.53134829414712\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [1]:\n",
    "    for label_perc in [.1]:\n",
    "        load_pretrained_(models, [sampling_perc])\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune_no_cntxt_scaling\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                n_max_elements=None,\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_no_cntxt_scaling epoch: 10 val_loss: 20.53134829414712 val_acc: 0.9317950458092976\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_flat ---\n",
      "\n",
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_flat best epoch: 9 val_loss: 22.130947111742675\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [1]:\n",
    "    for label_perc in [0.1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune_flat\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                is_neg_consistency=True,\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                 get_lambda_unsup=lambda: 1,\n",
    "                                                 get_lambda_sup=lambda: 1,\n",
    "                                                 get_lambda_neg_cons=lambda: 1,\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total # label perc is lower ebcause cocnat to test\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_flat epoch: 9 val_loss: 22.130947111742675 val_acc: 0.9423142178486597\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "Note that no get_lambda_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading 100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune ---\n",
      "\n",
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune best epoch: 40 val_loss: 15.338547741877333\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [1]:\n",
    "    for label_perc in [.1]:\n",
    "        load_pretrained_(models, [sampling_perc])\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune epoch: 40 val_loss: 15.338547741877333 val_acc: 0.9704784526637258\n"
     ]
    }
   ],
   "source": [
    "#0.9644 with both supervised and unsupervised with finetuning\n",
    "\n",
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Sampling Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 5%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "5%har/transformer_gnp_large_shared_bottleneck best epoch: 98 val_loss: -2.164531707763672\n",
      "\n",
      "--- Loading 5%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "5%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 30 val_loss: 74.09746773622382\n",
      "\n",
      "--- Loading 10%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "10%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -2.98053588682008\n",
      "\n",
      "--- Loading 10%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "10%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 22 val_loss: 59.960932497659456\n",
      "\n",
      "--- Loading 30%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "30%har/transformer_gnp_large_shared_bottleneck best epoch: 87 val_loss: -4.5797279249117215\n",
      "\n",
      "--- Loading 30%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "30%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 49 val_loss: 15.669058653148992\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 58 val_loss: 14.07143764282268\n",
      "\n",
      "--- Loading 70%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "70%har/transformer_gnp_large_shared_bottleneck best epoch: 98 val_loss: -7.103462460897501\n",
      "\n",
      "--- Loading 70%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "70%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 42 val_loss: 11.97455449099455\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n",
      "\n",
      "--- Loading 100%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "100%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 13 val_loss: 11.608399493758219\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in sampling_percentages:\n",
    "    for label_perc in [1]:\n",
    "        load_pretrained_(models, [sampling_perc])\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune_sup\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"supervised\",\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 30 val_loss: 74.09746773622382 val_acc: 0.7404139803189684\n",
      "10%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 22 val_loss: 59.960932497659456 val_acc: 0.8282999660671869\n",
      "30%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 49 val_loss: 15.669058653148992 val_acc: 0.9501187648456056\n",
      "50%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 58 val_loss: 14.07143764282268 val_acc: 0.9609772650152698\n",
      "70%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 42 val_loss: 11.97455449099455 val_acc: 0.9691211401425178\n",
      "100%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 13 val_loss: 11.608399493758219 val_acc: 0.9711571089243298\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Label Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_600%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_600%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 59 val_loss: 132.5539856855045\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_1200%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_1200%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 43 val_loss: 121.13525029809587\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_1%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_1%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 18 val_loss: 57.958797629582584\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_5%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_5%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 6 val_loss: 41.30282708107515\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 13 val_loss: 19.129878084579484\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_30%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_30%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 18 val_loss: 19.8407337821699\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_50%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_50%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 44 val_loss: 15.392488726203952\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 50%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup ---\n",
      "\n",
      "50%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup best epoch: 58 val_loss: 14.07143764282268\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in label_percentages:\n",
    "        is_retrain = IS_RETRAIN\n",
    "        if label_perc == 1: #already computed previous cell\n",
    "            is_retrain = False\n",
    "        \n",
    "        load_pretrained_(models, [sampling_perc])\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune_sup\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                label_perc=(label_perc * data_train.n_train)/data_train.n_total\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50%har_600%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 59 val_loss: 132.5539856855045 val_acc: 0.4842212419409569\n",
      "50%har_1200%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 43 val_loss: 121.13525029809587 val_acc: 0.496776382762131\n",
      "50%har_1%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 18 val_loss: 57.958797629582584 val_acc: 0.848320325755005\n",
      "50%har_5%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 6 val_loss: 41.30282708107515 val_acc: 0.9053274516457415\n",
      "50%har_10%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 13 val_loss: 19.129878084579484 val_acc: 0.9389209365456397\n",
      "50%har_30%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 18 val_loss: 19.8407337821699 val_acc: 0.9453681710213777\n",
      "50%har_50%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 44 val_loss: 15.392488726203952 val_acc: 0.9548693586698337\n",
      "50%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck_finetune_sup epoch: 58 val_loss: 14.07143764282268 val_acc: 0.9609772650152698\n"
     ]
    }
   ],
   "source": [
    "# if bad has to try freezing again and smaller params\n",
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.9304 best without n max elements\n",
    "* 0.9277: jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies\n",
    "* 0.9857 : jsd | 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze\n",
    "* 0.9623 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.5] | linear interpolator (1,5)\n",
    "\n",
    "\n",
    "* 0.9671 : jsd | no scale ? | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.1,0.5]\n",
    "* 0.9365 : jsd | no scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.01,0.5] | linear interpolator\n",
    "* 0.9824 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.5] | linear interpolator\n",
    "\n",
    "\n",
    "* 0.9844 : jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze\n",
    "* 0.9817 : jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | cntxt [0.01,0.5]\n",
    "\n",
    "\n",
    "* 0.9627 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.5]\n",
    "* 0.9572 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.9] | linear interpolator\n",
    "\n",
    "\n",
    "* 0.9321: jsd | no 0.1 scale | n_max_elements | 100 sampels\n",
    "* 0.9365: jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.1 entropies\n",
    "\n",
    "\n",
    "* 0.9450 : jsd | 0.2 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.01,0.5]\n",
    "* 0.9315 : jsd | no scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.01,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 5%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "5%har/transformer_gnp_large_shared_bottleneck best epoch: 98 val_loss: -2.164531707763672\n",
      "\n",
      "--- Loading 10%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "10%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -2.98053588682008\n",
      "\n",
      "--- Loading 30%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "30%har/transformer_gnp_large_shared_bottleneck best epoch: 87 val_loss: -4.5797279249117215\n",
      "\n",
      "--- Loading 50%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "50%har/transformer_gnp_large_shared_bottleneck best epoch: 72 val_loss: -5.531349883959131\n",
      "\n",
      "--- Loading 70%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "70%har/transformer_gnp_large_shared_bottleneck best epoch: 98 val_loss: -7.103462460897501\n",
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n"
     ]
    }
   ],
   "source": [
    "# load all transformers\n",
    "loaded_models = {}\n",
    "for sampling_perc in sampling_percentages:\n",
    "    for k, m in models.items():\n",
    "        if \"transformer\" not in k:\n",
    "            continue\n",
    "            \n",
    "        out = train_models_({\"{}%har\".format(int(sampling_perc*100)): \n",
    "                                            (None, None)}, \n",
    "                              {k :m },\n",
    "                               chckpnt_dirname=chckpnt_dirname,\n",
    "                               is_retrain=False)\n",
    "        \n",
    "        pretrained_model = out[list(out.keys())[0]].module_\n",
    "        model_dict = models[k.replace(\"transformer\", \"ssl_classifier\")].state_dict()\n",
    "        model_dict.update(pretrained_model.state_dict())\n",
    "        models[k.replace(\"transformer\", \"ssl_classifier\")].load_state_dict(model_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal, Categorical, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([0.2, 0.8], requires_grad=True)\n",
    "t2 = torch.tensor([0.7, 0.3], requires_grad=True)\n",
    "#torch.softmax(t2, -1)\n",
    "#torch.softmax(t1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = (t1 + t2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon_div(p1, p2):\n",
    "    p_avg = (p1 + p2) / 2\n",
    "    mask = (p_avg != 0).float()\n",
    "    # set to 0 p when M is 0 (because mean can only be 0 is vectors weree, but\n",
    "    # this is not the case due to numerical issues)\n",
    "    M = Categorical(probs=p_avg)\n",
    "    return ((kl_divergence(Categorical(probs=p1 * mask), M) +\n",
    "             kl_divergence(Categorical(probs=p2 * mask), M)) / 2)\n",
    "\n",
    "def yann_div(t1, t2):\n",
    "    M = (t1 + t2) / 2\n",
    "    return torch.min(kl_divergence(Categorical(probs=t1), Categorical(M)) + \n",
    "               kl_divergence(Categorical(probs=t2), Categorical(M)))\n",
    "\n",
    "def csiszar_dist(t1, t2):\n",
    "    M = (t1 + t2) / 2\n",
    "    return ((kl_divergence(Categorical(M), Categorical(probs=t1)\n",
    "                ) + kl_divergence(Categorical(M), Categorical(probs=t2)))/2)#**0.5\n",
    "\n",
    "def total_var(t1, t2):\n",
    "    return (t1 - t2).abs().sum(-1) / 2\n",
    "\n",
    "def bhattacharyya_dist(t1, t2):\n",
    "    return -torch.log((t1 * t2).sqrt().sum(-1))\n",
    "\n",
    "def hellinger_dist(t1, t2):\n",
    "    return (t1.sqrt() - t2.sqrt()).pow(2).sum(-1).sqrt() / (2**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t1,t2 in [([0., 1], [1, 0.]), \n",
    "              ([0.5, 0.5], [0.4, 0.6]), \n",
    "              ([0.5, 0.5], [0.5, 0.5]), \n",
    "              ([0.4, 0.6], [0.3, 0.7]), \n",
    "              ([1-1e-50, 1e-50], [1e-50, 1-1e-50]), \n",
    "              ([0.1, 0.1, 0.8], [0.2, 0.2, 0.6]), \n",
    "              ([0.1, 0.1, 0.8], [0.6, 0.2, 0.2])]:\n",
    "    print()\n",
    "    print(t1, t2)\n",
    "    print(\"yd\", yann_div(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"cd\", csiszar_dist(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"tv\", total_var(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"jsd\", jensen_shannon_div(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"bd\", bhattacharyya_dist(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"hd\", hellinger_dist(torch.tensor(t1), torch.tensor(t2)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
