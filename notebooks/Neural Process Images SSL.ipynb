{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Process Images SSL\n",
    "\n",
    "Last Update : 29 July 2019\n",
    "\n",
    "**Aim**: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 8\n",
    "# Nota Bene : notebooks don't deallocate GPU memory\n",
    "IS_FORCE_CPU = True # can also be set in the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/conv\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
       ".prompt display:none;}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# CENTER PLOTS\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\" <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
    ".prompt display:none;}  </style>\"\"\"))\n",
    "\n",
    "import os\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "    \n",
    "import sys\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "\n",
    "SVHN \n",
    "MNIST\n",
    "CELEBA\n",
    "CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ntbks_add_data as adddata \n",
    "from functools import partial\n",
    "from utils.data.ssldata import get_dataset, get_train_dev_test_ssl, make_ssl_dataset_\n",
    "from utils.data.helpers import train_dev_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "svhn_train, _, svhn_test = get_train_dev_test_ssl(\"svhn\", dev_size=0)\n",
    "#cifar10_train, _, cifar10_test = get_train_dev_test_ssl(\"cifar10\", dev_size=0)\n",
    "mnist_train, _, mnist_test = get_train_dev_test_ssl(\"mnist\", dev_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skssl.transformers.neuralproc.datasplit import GridCntxtTrgtGetter, RandomMasker, no_masker, half_masker\n",
    "from utils.data.tsdata import get_timeseries_dataset, SparseMultiTimeSeriesDataset\n",
    "\n",
    "get_cntxt_trgt_test = GridCntxtTrgtGetter(context_masker=RandomMasker(min_nnz=0.01, max_nnz=0.50),\n",
    "                                     target_masker=no_masker,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt_feat = GridCntxtTrgtGetter(context_masker=no_masker,\n",
    "                                     target_masker=no_masker,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt = GridCntxtTrgtGetter(context_masker=RandomMasker(min_nnz=0.01, max_nnz=0.50),\n",
    "                                 target_masker=RandomMasker(min_nnz=0.50, max_nnz=0.99),\n",
    "                                 is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "def cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=False, is_grided=False):\n",
    "    def mycollate(batch):\n",
    "        \n",
    "        if isinstance(batch[0][0], dict):\n",
    "            min_length = min([v.size(0) for b in batch for k,v in b[0].items() if \"X\" in k])\n",
    "            # chose first min_legth of each (assumes that randomized)\n",
    "\n",
    "            batch = [({k:v[:min_length, ...] for k,v in b[0].items()}, b[1]) for b in batch]        \n",
    "            collated = torch.utils.data.dataloader.default_collate(batch)\n",
    "        \n",
    "            X = collated[0][\"X\"]\n",
    "            y = collated[0][\"y\"]\n",
    "        else:       \n",
    "            collated = torch.utils.data.dataloader.default_collate(batch)\n",
    "            \n",
    "            X = collated[0]\n",
    "            y = None\n",
    "            collated[0] = dict()\n",
    "        \n",
    "        if is_repeat_batch:\n",
    "            X = torch.cat([X,X], dim=0)\n",
    "            if y is not None:\n",
    "                y = torch.cat([y,y], dim=0)\n",
    "            collated[1] = torch.cat([collated[1], collated[1]], dim=0) # targets\n",
    "        \n",
    "        if is_grided:\n",
    "            collated = (dict(), collated[1])\n",
    "            collated[0][\"X\"], collated[0][\"mask_context\"], collated[0][\"mask_target\"] = get_cntxt_trgt(X, y, \n",
    "                                                                                                       is_grided=True)\n",
    "            \n",
    "        else:\n",
    "            collated[0][\"X\"], collated[0][\"y\"], collated[0][\"X_trgt\"], collated[0][\"y_trgt\"] = get_cntxt_trgt(X, y)\n",
    "            \n",
    "        \n",
    "        return collated\n",
    "    return mycollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dict(svhn=(svhn_train, svhn_test), \n",
    "                #cifar10=(cifar10_train, cifar10_test), \n",
    "                mnist=(mnist_train, mnist_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_specific_kwargs = dict(svhn=dict(y_dim=svhn_train.shape[0]), \n",
    "                            #cifar10=dict(y_dim=cifar10_train.shape[0]),\n",
    "                            mnist=dict(y_dim=mnist_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DIM = 2  # 2D spatial input \n",
    "#Y_DIM = data.shape[0]\n",
    "N_TARGETS = 10\n",
    "\n",
    "#label_percentages = [N_TARGETS, N_TARGETS*2, 0.01, 0.05, 0.1, 0.3, 0.5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skssl.transformers import AttentiveNeuralProcess, NeuralProcessLoss, GridConvNeuralProcess, GridNeuralProcessSSLLoss\n",
    "from skssl.predefined import UnetCNN, CNN, SelfAttention, MLP, SelfAttention, SinusoidalEncodings, merge_flat_input\n",
    "from skssl.transformers.neuralproc.datasplit import precomputed_cntxt_trgt_split\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "models = {}\n",
    "\n",
    "m_clf = lambda y_dim: partial(GridConvNeuralProcess,\n",
    "                              y_dim=y_dim,\n",
    "                              r_dim=64,\n",
    "                              output_range=(0, 1),\n",
    "                              is_clf_features=False,\n",
    "                              Classifier=partial(MLP, input_size=256, output_size=N_TARGETS,\n",
    "                                                 dropout=0.,\n",
    "                                                 hidden_size=128, n_hidden_layers=3, is_res=True),\n",
    "                              TmpSelfAttn=partial(\n",
    "                                  UnetCNN,\n",
    "                                  Conv=torch.nn.Conv2d,\n",
    "                                  Pool=torch.nn.MaxPool2d,\n",
    "                                  upsample_mode=\"bilinear\",\n",
    "                                  n_layers=18,\n",
    "                                  is_double_conv=True,\n",
    "                                  is_depth_separable=True,\n",
    "                                  Normalization=torch.nn.BatchNorm2d,\n",
    "                                  is_chan_last=True,\n",
    "                                  bottleneck=None,\n",
    "                                  kernel_size=7,\n",
    "                                  max_nchannels=256,\n",
    "                                  is_force_same_bottleneck=True,\n",
    "                                  _is_summary=True,\n",
    "\n",
    "                              ))\n",
    "\n",
    "models[\"ssl_classifier_gnp_large_unet\"] = m_clf\n",
    "\n",
    "m_trnsf = lambda y_dim: partial(GridConvNeuralProcess,\n",
    "                                y_dim=y_dim,\n",
    "                                r_dim=64,\n",
    "                                output_range=(0, 1),\n",
    "                                Classifier=None,\n",
    "                                TmpSelfAttn=partial(\n",
    "                                    UnetCNN,\n",
    "                                    Conv=torch.nn.Conv2d,\n",
    "                                    Pool=torch.nn.MaxPool2d,\n",
    "                                    upsample_mode=\"bilinear\",\n",
    "                                    n_layers=18,\n",
    "                                    is_double_conv=True,\n",
    "                                    is_depth_separable=True,\n",
    "                                    Normalization=torch.nn.BatchNorm2d,\n",
    "                                    is_chan_last=True,\n",
    "                                    bottleneck=None,\n",
    "                                    kernel_size=7,\n",
    "                                    max_nchannels=256,\n",
    "                                    is_force_same_bottleneck=True,\n",
    "                                    _is_summary=True)\n",
    "                                )\n",
    "\n",
    "\n",
    "models[\"transformer_gnp_large_unet\"] = m_trnsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssl_classifier_gnp_large_unet - N Param: 1255825\n",
      "transformer_gnp_large_unet - N Param: 1188615\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "for k, v in models.items():\n",
    "    print(k, \"- N Param:\", count_parameters(v(y_dim=3)()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_(models, data_name, datasets, data_specific_kwargs):\n",
    "\n",
    "    # ALREADY INITALIZE TO BE ABLE TO LOAD\n",
    "    models[\"ssl_classifier_gnp_large_unet\"] = m_clf(**data_specific_kwargs[data_name])()\n",
    "    models[\"transformer_gnp_large_unet\"] = m_trnsf(**data_specific_kwargs[data_name])()\n",
    "\n",
    "    # load all transformers\n",
    "    loaded_models = {}\n",
    "    for k, m in models.items():\n",
    "        if \"transformer\" not in k:\n",
    "            continue\n",
    "\n",
    "        out = train_models_({data_name:datasets[data_name]}, {k :m },\n",
    "                            chckpnt_dirname=chckpnt_dirname,\n",
    "                            is_retrain=False,\n",
    "                           seed=None)\n",
    "\n",
    "        pretrained_model = out[list(out.keys())[0]].module_\n",
    "        model_dict = models[k.replace(\"transformer\", \"ssl_classifier\")].state_dict()\n",
    "        model_dict.update(pretrained_model.state_dict())\n",
    "        models[k.replace(\"transformer\", \"ssl_classifier\")].load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntbks_helpers import train_models_\n",
    "from skorch.dataset import CVSplit\n",
    "from utils.data.ssldata import get_train_dev_test_ssl\n",
    "\n",
    "N_EPOCHS = 100 \n",
    "BATCH_SIZE = 32\n",
    "IS_RETRAIN = False # if false load precomputed\n",
    "chckpnt_dirname=\"results/notebooks/neural_process_images/\"\n",
    "\n",
    "from skssl.utils.helpers import HyperparameterInterpolator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading svhn/transformer_gnp_large_unet ---\n",
      "\n",
      "svhn/transformer_gnp_large_unet best epoch: 6 val_loss: -3.949710527958134\n",
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/test_32x32.mat\n",
      "\n",
      "--- Loading svhn/ssl_classifier_gnp_large_unet_finetune ---\n",
      "\n",
      "svhn/ssl_classifier_gnp_large_unet_finetune best epoch: 4 val_loss: 0.8266690941624797\n",
      "\n",
      "--- Loading mnist/transformer_gnp_large_unet ---\n",
      "\n",
      "mnist/transformer_gnp_large_unet best epoch: 9 val_loss: -1.2522213287353516\n",
      "\n",
      "--- Loading mnist/ssl_classifier_gnp_large_unet_finetune ---\n",
      "\n",
      "mnist/ssl_classifier_gnp_large_unet_finetune best epoch: 7 val_loss: 0.42914011276960373\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for data_name, (data_train, data_test) in datasets.items():\n",
    "        \n",
    "    load_pretrained_(models, data_name, datasets, data_specific_kwargs)\n",
    "    \n",
    "\n",
    "    data_train, _, data_test = get_train_dev_test_ssl(data_name, dev_size=0, is_augment=True)\n",
    "\n",
    "    # add test as unlabeled data\n",
    "    data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "    if data_name == \"mnist\":\n",
    "        data_train.data = torch.from_numpy(data_train.data) # mnist to have data as tensor\n",
    "        \n",
    "    data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "    \n",
    "    is_ssl_only = False\n",
    "    if is_ssl_only:\n",
    "        idcs = data_train.targets != -1\n",
    "        data_train.data = data_train.data[torch.from_numpy(idcs)]\n",
    "        data_train.targets = data_train.targets[idcs]\n",
    "        sfx_ssl = \"_ssl_only\"\n",
    "    else:\n",
    "        sfx_ssl = \"\"\n",
    "\n",
    "    n_max_elements = 1024\n",
    "\n",
    "    label_perc = (data_train.targets != -1).sum() / len(data_train.targets)\n",
    "    sfx_lab_perc = \"\" if label_perc is None else \"_labperc\"\n",
    "\n",
    "    from skssl.utils.helpers import HyperparameterInterpolator\n",
    "    n_steps_per_epoch = len(data_train) // BATCH_SIZE\n",
    "    get_lambda_clf = HyperparameterInterpolator(1, 50, N_EPOCHS * n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "    data_trainers.update(train_models_({data_name: (data_train, data_test)},\n",
    "                                       {k + \"_finetune\": m for k, m in models.items()\n",
    "                                        if \"ssl_classifier\" in k},\n",
    "                                       criterion=partial(GridNeuralProcessSSLLoss,\n",
    "                                                         n_max_elements=n_max_elements,\n",
    "                                                         label_perc=label_perc,\n",
    "                                                         is_ssl_only=False,\n",
    "                                                         get_lambda_unsup=lambda: 1,\n",
    "                                                         get_lambda_ent=lambda: 0.5,\n",
    "                                                         get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                         get_lambda_neg_cons=lambda: 0.5,\n",
    "                                                         min_sigma=0.1\n",
    "                                                         ),\n",
    "                                       patience=15,\n",
    "                                       chckpnt_dirname=chckpnt_dirname,\n",
    "                                       max_epochs=N_EPOCHS,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       is_retrain=IS_RETRAIN,\n",
    "                                       is_monitor_acc=True,\n",
    "                                       callbacks=[],\n",
    "                                       iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_grided=True, is_repeat_batch=True),\n",
    "                                       iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat, is_grided=True),\n",
    "                                       mode=\"transformer\",\n",
    "                                       ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svhn/ssl_classifier_gnp_large_unet_finetune epoch: 16 val_loss: 1.2152564730758526 val_acc: 0.8593269821757836\n",
      "mnist/ssl_classifier_gnp_large_unet_finetune epoch: 28 val_loss: 0.561877640029043 val_acc: 0.9627\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    if \"transformer\" in k:\n",
    "        continue\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_acc_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svhn/ssl_classifier_gnp_large_unet_finetune epoch: 16 val_loss: 1.2152564730758526 val_acc: 0.8593269821757836\n",
      "cifar10/ssl_classifier_gnp_large_unet_finetune epoch: 39 val_loss: 1.659980283355713 val_acc: 0.7259\n",
      "mnist/ssl_classifier_gnp_large_unet_finetune epoch: 28 val_loss: 0.561877640029043 val_acc: 0.9627\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    if \"transformer\" in k:\n",
    "        continue\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_acc_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading svhn/transformer_gnp_large_unet ---\n",
      "\n",
      "svhn/transformer_gnp_large_unet best epoch: 6 val_loss: -3.949710527958134\n",
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/test_32x32.mat\n",
      "\n",
      "--- Loading svhn/ssl_classifier_gnp_large_unet_finetune_sup_vanilla ---\n",
      "\n",
      "svhn/ssl_classifier_gnp_large_unet_finetune_sup_vanilla best epoch: 2 val_loss: 0.7766240948391019\n",
      "\n",
      "--- Loading mnist/transformer_gnp_large_unet ---\n",
      "\n",
      "mnist/transformer_gnp_large_unet best epoch: 9 val_loss: -1.2522213287353516\n",
      "\n",
      "--- Loading mnist/ssl_classifier_gnp_large_unet_finetune_sup_vanilla ---\n",
      "\n",
      "mnist/ssl_classifier_gnp_large_unet_finetune_sup_vanilla best epoch: 1 val_loss: 0.3784724204778671\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for data_name, (data_train, data_test) in datasets.items():\n",
    "        \n",
    "    load_pretrained_(models, data_name, datasets, data_specific_kwargs)\n",
    "    \n",
    "\n",
    "    data_train, _, data_test = get_train_dev_test_ssl(data_name, dev_size=0, is_augment=True)\n",
    "\n",
    "    # add test as unlabeled data\n",
    "    data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "    if data_name == \"mnist\":\n",
    "        data_train.data = torch.from_numpy(data_train.data) # mnist to have data as tensor\n",
    "        \n",
    "    data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "    \n",
    "    is_ssl_only = False\n",
    "    if is_ssl_only:\n",
    "        idcs = data_train.targets != -1\n",
    "        data_train.data = data_train.data[torch.from_numpy(idcs)]\n",
    "        data_train.targets = data_train.targets[idcs]\n",
    "        sfx_ssl = \"_ssl_only\"\n",
    "    else:\n",
    "        sfx_ssl = \"\"\n",
    "\n",
    "    n_max_elements = 1024\n",
    "\n",
    "    label_perc = (data_train.targets != -1).sum() / len(data_train.targets)\n",
    "    sfx_lab_perc = \"\" if label_perc is None else \"_labperc\"\n",
    "\n",
    "    from skssl.utils.helpers import HyperparameterInterpolator\n",
    "    n_steps_per_epoch = len(data_train) // BATCH_SIZE\n",
    "    get_lambda_clf = HyperparameterInterpolator(1, 50, N_EPOCHS * n_steps_per_epoch, mode=\"linear\")\n",
    "\n",
    "    data_trainers.update(train_models_({data_name: (data_train, data_test)},\n",
    "                                       {k + \"_finetune_sup_vanilla\": m for k, m in models.items()\n",
    "                                        if \"ssl_classifier\" in k},\n",
    "                                       criterion=partial(GridNeuralProcessSSLLoss,\n",
    "                                                         n_max_elements=n_max_elements,\n",
    "                                                         label_perc=label_perc,\n",
    "                                                         is_ssl_only=False,\n",
    "                                                         get_lambda_unsup=lambda: 1,\n",
    "                                                         get_lambda_ent=lambda: 0.5,\n",
    "                                                         get_lambda_sup=lambda: get_lambda_clf(True),\n",
    "                                                         get_lambda_neg_cons=lambda: 0.5,\n",
    "                                                         min_sigma=0.1\n",
    "                                                         ),\n",
    "                                       patience=15,\n",
    "                                       chckpnt_dirname=chckpnt_dirname,\n",
    "                                       max_epochs=N_EPOCHS,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       is_retrain=IS_RETRAIN,\n",
    "                                       is_monitor_acc=True,\n",
    "                                       callbacks=[],\n",
    "                                       iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt, is_grided=True, is_repeat_batch=True),\n",
    "                                       iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat, is_grided=True),\n",
    "                                       mode=\"transformer\",\n",
    "                                       ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svhn/ssl_classifier_gnp_large_unet_finetune_sup_vanilla epoch: 2 val_loss: 0.7766240948391019 val_acc: 0.7884142593730793\n",
      "mnist/ssl_classifier_gnp_large_unet_finetune_sup_vanilla epoch: 1 val_loss: 0.3784724204778671 val_acc: 0.9036\n"
     ]
    }
   ],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    if \"transformer\" in k:\n",
    "        continue\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_acc_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_data(chckpnt_dirname, data_trainers):\n",
    "    for k, trainer in data_trainers.items():\n",
    "        model_name = k.split(\"/\")[1]\n",
    "        data_name = k.split(\"/\")[0]\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(data_name, dev_size=0)\n",
    "        trainer.set_params(iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat, is_grided=True, is_repeat_batch=False),\n",
    "                           iterator_valid__shuffle=False) # make sure not shuffling because only transforming X\n",
    "        if torch.cuda.is_available():\n",
    "            trainer.module_.cuda()\n",
    "        transformed_data_train = trainer.transform(data_train)\n",
    "        transformed_data_test = trainer.transform(data_test)\n",
    "        np.save(chckpnt_dirname+k+\"/transformed_data_train.npy\", transformed_data_train, allow_pickle=False)\n",
    "        np.save(chckpnt_dirname+k+\"/transformed_data_test.npy\", transformed_data_test, allow_pickle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /conv/utils/data/../../data/SVHN/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#save_transformed_data(chckpnt_dirname, data_trainers)\n",
    "# saving all transformed data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
